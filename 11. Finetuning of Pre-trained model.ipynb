{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning of Pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have shown how can we used and finetune a pre-trained model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    }
   ],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_6 (None, None, None, 3) (None, None, None, 3)\n",
      "1 conv2d_471 (None, None, None, 3) (None, None, None, 32)\n",
      "2 batch_normalization_471 (None, None, None, 32) (None, None, None, 32)\n",
      "3 activation_471 (None, None, None, 32) (None, None, None, 32)\n",
      "4 conv2d_472 (None, None, None, 32) (None, None, None, 32)\n",
      "5 batch_normalization_472 (None, None, None, 32) (None, None, None, 32)\n",
      "6 activation_472 (None, None, None, 32) (None, None, None, 32)\n",
      "7 conv2d_473 (None, None, None, 32) (None, None, None, 64)\n",
      "8 batch_normalization_473 (None, None, None, 64) (None, None, None, 64)\n",
      "9 activation_473 (None, None, None, 64) (None, None, None, 64)\n",
      "10 max_pooling2d_21 (None, None, None, 64) (None, None, None, 64)\n",
      "11 conv2d_474 (None, None, None, 64) (None, None, None, 80)\n",
      "12 batch_normalization_474 (None, None, None, 80) (None, None, None, 80)\n",
      "13 activation_474 (None, None, None, 80) (None, None, None, 80)\n",
      "14 conv2d_475 (None, None, None, 80) (None, None, None, 192)\n",
      "15 batch_normalization_475 (None, None, None, 192) (None, None, None, 192)\n",
      "16 activation_475 (None, None, None, 192) (None, None, None, 192)\n",
      "17 max_pooling2d_22 (None, None, None, 192) (None, None, None, 192)\n",
      "18 conv2d_479 (None, None, None, 192) (None, None, None, 64)\n",
      "19 batch_normalization_479 (None, None, None, 64) (None, None, None, 64)\n",
      "20 activation_479 (None, None, None, 64) (None, None, None, 64)\n",
      "21 conv2d_477 (None, None, None, 192) (None, None, None, 48)\n",
      "22 conv2d_480 (None, None, None, 64) (None, None, None, 96)\n",
      "23 batch_normalization_477 (None, None, None, 48) (None, None, None, 48)\n",
      "24 batch_normalization_480 (None, None, None, 96) (None, None, None, 96)\n",
      "25 activation_477 (None, None, None, 48) (None, None, None, 48)\n",
      "26 activation_480 (None, None, None, 96) (None, None, None, 96)\n",
      "27 average_pooling2d_46 (None, None, None, 192) (None, None, None, 192)\n",
      "28 conv2d_476 (None, None, None, 192) (None, None, None, 64)\n",
      "29 conv2d_478 (None, None, None, 48) (None, None, None, 64)\n",
      "30 conv2d_481 (None, None, None, 96) (None, None, None, 96)\n",
      "31 conv2d_482 (None, None, None, 192) (None, None, None, 32)\n",
      "32 batch_normalization_476 (None, None, None, 64) (None, None, None, 64)\n",
      "33 batch_normalization_478 (None, None, None, 64) (None, None, None, 64)\n",
      "34 batch_normalization_481 (None, None, None, 96) (None, None, None, 96)\n",
      "35 batch_normalization_482 (None, None, None, 32) (None, None, None, 32)\n",
      "36 activation_476 (None, None, None, 64) (None, None, None, 64)\n",
      "37 activation_478 (None, None, None, 64) (None, None, None, 64)\n",
      "38 activation_481 (None, None, None, 96) (None, None, None, 96)\n",
      "39 activation_482 (None, None, None, 32) (None, None, None, 32)\n",
      "40 mixed0 [(None, None, None, 64), (None, None, None, 64), (None, None, None, 96), (None, None, None, 32)] (None, None, None, 256)\n",
      "41 conv2d_486 (None, None, None, 256) (None, None, None, 64)\n",
      "42 batch_normalization_486 (None, None, None, 64) (None, None, None, 64)\n",
      "43 activation_486 (None, None, None, 64) (None, None, None, 64)\n",
      "44 conv2d_484 (None, None, None, 256) (None, None, None, 48)\n",
      "45 conv2d_487 (None, None, None, 64) (None, None, None, 96)\n",
      "46 batch_normalization_484 (None, None, None, 48) (None, None, None, 48)\n",
      "47 batch_normalization_487 (None, None, None, 96) (None, None, None, 96)\n",
      "48 activation_484 (None, None, None, 48) (None, None, None, 48)\n",
      "49 activation_487 (None, None, None, 96) (None, None, None, 96)\n",
      "50 average_pooling2d_47 (None, None, None, 256) (None, None, None, 256)\n",
      "51 conv2d_483 (None, None, None, 256) (None, None, None, 64)\n",
      "52 conv2d_485 (None, None, None, 48) (None, None, None, 64)\n",
      "53 conv2d_488 (None, None, None, 96) (None, None, None, 96)\n",
      "54 conv2d_489 (None, None, None, 256) (None, None, None, 64)\n",
      "55 batch_normalization_483 (None, None, None, 64) (None, None, None, 64)\n",
      "56 batch_normalization_485 (None, None, None, 64) (None, None, None, 64)\n",
      "57 batch_normalization_488 (None, None, None, 96) (None, None, None, 96)\n",
      "58 batch_normalization_489 (None, None, None, 64) (None, None, None, 64)\n",
      "59 activation_483 (None, None, None, 64) (None, None, None, 64)\n",
      "60 activation_485 (None, None, None, 64) (None, None, None, 64)\n",
      "61 activation_488 (None, None, None, 96) (None, None, None, 96)\n",
      "62 activation_489 (None, None, None, 64) (None, None, None, 64)\n",
      "63 mixed1 [(None, None, None, 64), (None, None, None, 64), (None, None, None, 96), (None, None, None, 64)] (None, None, None, 288)\n",
      "64 conv2d_493 (None, None, None, 288) (None, None, None, 64)\n",
      "65 batch_normalization_493 (None, None, None, 64) (None, None, None, 64)\n",
      "66 activation_493 (None, None, None, 64) (None, None, None, 64)\n",
      "67 conv2d_491 (None, None, None, 288) (None, None, None, 48)\n",
      "68 conv2d_494 (None, None, None, 64) (None, None, None, 96)\n",
      "69 batch_normalization_491 (None, None, None, 48) (None, None, None, 48)\n",
      "70 batch_normalization_494 (None, None, None, 96) (None, None, None, 96)\n",
      "71 activation_491 (None, None, None, 48) (None, None, None, 48)\n",
      "72 activation_494 (None, None, None, 96) (None, None, None, 96)\n",
      "73 average_pooling2d_48 (None, None, None, 288) (None, None, None, 288)\n",
      "74 conv2d_490 (None, None, None, 288) (None, None, None, 64)\n",
      "75 conv2d_492 (None, None, None, 48) (None, None, None, 64)\n",
      "76 conv2d_495 (None, None, None, 96) (None, None, None, 96)\n",
      "77 conv2d_496 (None, None, None, 288) (None, None, None, 64)\n",
      "78 batch_normalization_490 (None, None, None, 64) (None, None, None, 64)\n",
      "79 batch_normalization_492 (None, None, None, 64) (None, None, None, 64)\n",
      "80 batch_normalization_495 (None, None, None, 96) (None, None, None, 96)\n",
      "81 batch_normalization_496 (None, None, None, 64) (None, None, None, 64)\n",
      "82 activation_490 (None, None, None, 64) (None, None, None, 64)\n",
      "83 activation_492 (None, None, None, 64) (None, None, None, 64)\n",
      "84 activation_495 (None, None, None, 96) (None, None, None, 96)\n",
      "85 activation_496 (None, None, None, 64) (None, None, None, 64)\n",
      "86 mixed2 [(None, None, None, 64), (None, None, None, 64), (None, None, None, 96), (None, None, None, 64)] (None, None, None, 288)\n",
      "87 conv2d_498 (None, None, None, 288) (None, None, None, 64)\n",
      "88 batch_normalization_498 (None, None, None, 64) (None, None, None, 64)\n",
      "89 activation_498 (None, None, None, 64) (None, None, None, 64)\n",
      "90 conv2d_499 (None, None, None, 64) (None, None, None, 96)\n",
      "91 batch_normalization_499 (None, None, None, 96) (None, None, None, 96)\n",
      "92 activation_499 (None, None, None, 96) (None, None, None, 96)\n",
      "93 conv2d_497 (None, None, None, 288) (None, None, None, 384)\n",
      "94 conv2d_500 (None, None, None, 96) (None, None, None, 96)\n",
      "95 batch_normalization_497 (None, None, None, 384) (None, None, None, 384)\n",
      "96 batch_normalization_500 (None, None, None, 96) (None, None, None, 96)\n",
      "97 activation_497 (None, None, None, 384) (None, None, None, 384)\n",
      "98 activation_500 (None, None, None, 96) (None, None, None, 96)\n",
      "99 max_pooling2d_23 (None, None, None, 288) (None, None, None, 288)\n",
      "100 mixed3 [(None, None, None, 384), (None, None, None, 96), (None, None, None, 288)] (None, None, None, 768)\n",
      "101 conv2d_505 (None, None, None, 768) (None, None, None, 128)\n",
      "102 batch_normalization_505 (None, None, None, 128) (None, None, None, 128)\n",
      "103 activation_505 (None, None, None, 128) (None, None, None, 128)\n",
      "104 conv2d_506 (None, None, None, 128) (None, None, None, 128)\n",
      "105 batch_normalization_506 (None, None, None, 128) (None, None, None, 128)\n",
      "106 activation_506 (None, None, None, 128) (None, None, None, 128)\n",
      "107 conv2d_502 (None, None, None, 768) (None, None, None, 128)\n",
      "108 conv2d_507 (None, None, None, 128) (None, None, None, 128)\n",
      "109 batch_normalization_502 (None, None, None, 128) (None, None, None, 128)\n",
      "110 batch_normalization_507 (None, None, None, 128) (None, None, None, 128)\n",
      "111 activation_502 (None, None, None, 128) (None, None, None, 128)\n",
      "112 activation_507 (None, None, None, 128) (None, None, None, 128)\n",
      "113 conv2d_503 (None, None, None, 128) (None, None, None, 128)\n",
      "114 conv2d_508 (None, None, None, 128) (None, None, None, 128)\n",
      "115 batch_normalization_503 (None, None, None, 128) (None, None, None, 128)\n",
      "116 batch_normalization_508 (None, None, None, 128) (None, None, None, 128)\n",
      "117 activation_503 (None, None, None, 128) (None, None, None, 128)\n",
      "118 activation_508 (None, None, None, 128) (None, None, None, 128)\n",
      "119 average_pooling2d_49 (None, None, None, 768) (None, None, None, 768)\n",
      "120 conv2d_501 (None, None, None, 768) (None, None, None, 192)\n",
      "121 conv2d_504 (None, None, None, 128) (None, None, None, 192)\n",
      "122 conv2d_509 (None, None, None, 128) (None, None, None, 192)\n",
      "123 conv2d_510 (None, None, None, 768) (None, None, None, 192)\n",
      "124 batch_normalization_501 (None, None, None, 192) (None, None, None, 192)\n",
      "125 batch_normalization_504 (None, None, None, 192) (None, None, None, 192)\n",
      "126 batch_normalization_509 (None, None, None, 192) (None, None, None, 192)\n",
      "127 batch_normalization_510 (None, None, None, 192) (None, None, None, 192)\n",
      "128 activation_501 (None, None, None, 192) (None, None, None, 192)\n",
      "129 activation_504 (None, None, None, 192) (None, None, None, 192)\n",
      "130 activation_509 (None, None, None, 192) (None, None, None, 192)\n",
      "131 activation_510 (None, None, None, 192) (None, None, None, 192)\n",
      "132 mixed4 [(None, None, None, 192), (None, None, None, 192), (None, None, None, 192), (None, None, None, 192)] (None, None, None, 768)\n",
      "133 conv2d_515 (None, None, None, 768) (None, None, None, 160)\n",
      "134 batch_normalization_515 (None, None, None, 160) (None, None, None, 160)\n",
      "135 activation_515 (None, None, None, 160) (None, None, None, 160)\n",
      "136 conv2d_516 (None, None, None, 160) (None, None, None, 160)\n",
      "137 batch_normalization_516 (None, None, None, 160) (None, None, None, 160)\n",
      "138 activation_516 (None, None, None, 160) (None, None, None, 160)\n",
      "139 conv2d_512 (None, None, None, 768) (None, None, None, 160)\n",
      "140 conv2d_517 (None, None, None, 160) (None, None, None, 160)\n",
      "141 batch_normalization_512 (None, None, None, 160) (None, None, None, 160)\n",
      "142 batch_normalization_517 (None, None, None, 160) (None, None, None, 160)\n",
      "143 activation_512 (None, None, None, 160) (None, None, None, 160)\n",
      "144 activation_517 (None, None, None, 160) (None, None, None, 160)\n",
      "145 conv2d_513 (None, None, None, 160) (None, None, None, 160)\n",
      "146 conv2d_518 (None, None, None, 160) (None, None, None, 160)\n",
      "147 batch_normalization_513 (None, None, None, 160) (None, None, None, 160)\n",
      "148 batch_normalization_518 (None, None, None, 160) (None, None, None, 160)\n",
      "149 activation_513 (None, None, None, 160) (None, None, None, 160)\n",
      "150 activation_518 (None, None, None, 160) (None, None, None, 160)\n",
      "151 average_pooling2d_50 (None, None, None, 768) (None, None, None, 768)\n",
      "152 conv2d_511 (None, None, None, 768) (None, None, None, 192)\n",
      "153 conv2d_514 (None, None, None, 160) (None, None, None, 192)\n",
      "154 conv2d_519 (None, None, None, 160) (None, None, None, 192)\n",
      "155 conv2d_520 (None, None, None, 768) (None, None, None, 192)\n",
      "156 batch_normalization_511 (None, None, None, 192) (None, None, None, 192)\n",
      "157 batch_normalization_514 (None, None, None, 192) (None, None, None, 192)\n",
      "158 batch_normalization_519 (None, None, None, 192) (None, None, None, 192)\n",
      "159 batch_normalization_520 (None, None, None, 192) (None, None, None, 192)\n",
      "160 activation_511 (None, None, None, 192) (None, None, None, 192)\n",
      "161 activation_514 (None, None, None, 192) (None, None, None, 192)\n",
      "162 activation_519 (None, None, None, 192) (None, None, None, 192)\n",
      "163 activation_520 (None, None, None, 192) (None, None, None, 192)\n",
      "164 mixed5 [(None, None, None, 192), (None, None, None, 192), (None, None, None, 192), (None, None, None, 192)] (None, None, None, 768)\n",
      "165 conv2d_525 (None, None, None, 768) (None, None, None, 160)\n",
      "166 batch_normalization_525 (None, None, None, 160) (None, None, None, 160)\n",
      "167 activation_525 (None, None, None, 160) (None, None, None, 160)\n",
      "168 conv2d_526 (None, None, None, 160) (None, None, None, 160)\n",
      "169 batch_normalization_526 (None, None, None, 160) (None, None, None, 160)\n",
      "170 activation_526 (None, None, None, 160) (None, None, None, 160)\n",
      "171 conv2d_522 (None, None, None, 768) (None, None, None, 160)\n",
      "172 conv2d_527 (None, None, None, 160) (None, None, None, 160)\n",
      "173 batch_normalization_522 (None, None, None, 160) (None, None, None, 160)\n",
      "174 batch_normalization_527 (None, None, None, 160) (None, None, None, 160)\n",
      "175 activation_522 (None, None, None, 160) (None, None, None, 160)\n",
      "176 activation_527 (None, None, None, 160) (None, None, None, 160)\n",
      "177 conv2d_523 (None, None, None, 160) (None, None, None, 160)\n",
      "178 conv2d_528 (None, None, None, 160) (None, None, None, 160)\n",
      "179 batch_normalization_523 (None, None, None, 160) (None, None, None, 160)\n",
      "180 batch_normalization_528 (None, None, None, 160) (None, None, None, 160)\n",
      "181 activation_523 (None, None, None, 160) (None, None, None, 160)\n",
      "182 activation_528 (None, None, None, 160) (None, None, None, 160)\n",
      "183 average_pooling2d_51 (None, None, None, 768) (None, None, None, 768)\n",
      "184 conv2d_521 (None, None, None, 768) (None, None, None, 192)\n",
      "185 conv2d_524 (None, None, None, 160) (None, None, None, 192)\n",
      "186 conv2d_529 (None, None, None, 160) (None, None, None, 192)\n",
      "187 conv2d_530 (None, None, None, 768) (None, None, None, 192)\n",
      "188 batch_normalization_521 (None, None, None, 192) (None, None, None, 192)\n",
      "189 batch_normalization_524 (None, None, None, 192) (None, None, None, 192)\n",
      "190 batch_normalization_529 (None, None, None, 192) (None, None, None, 192)\n",
      "191 batch_normalization_530 (None, None, None, 192) (None, None, None, 192)\n",
      "192 activation_521 (None, None, None, 192) (None, None, None, 192)\n",
      "193 activation_524 (None, None, None, 192) (None, None, None, 192)\n",
      "194 activation_529 (None, None, None, 192) (None, None, None, 192)\n",
      "195 activation_530 (None, None, None, 192) (None, None, None, 192)\n",
      "196 mixed6 [(None, None, None, 192), (None, None, None, 192), (None, None, None, 192), (None, None, None, 192)] (None, None, None, 768)\n",
      "197 conv2d_535 (None, None, None, 768) (None, None, None, 192)\n",
      "198 batch_normalization_535 (None, None, None, 192) (None, None, None, 192)\n",
      "199 activation_535 (None, None, None, 192) (None, None, None, 192)\n",
      "200 conv2d_536 (None, None, None, 192) (None, None, None, 192)\n",
      "201 batch_normalization_536 (None, None, None, 192) (None, None, None, 192)\n",
      "202 activation_536 (None, None, None, 192) (None, None, None, 192)\n",
      "203 conv2d_532 (None, None, None, 768) (None, None, None, 192)\n",
      "204 conv2d_537 (None, None, None, 192) (None, None, None, 192)\n",
      "205 batch_normalization_532 (None, None, None, 192) (None, None, None, 192)\n",
      "206 batch_normalization_537 (None, None, None, 192) (None, None, None, 192)\n",
      "207 activation_532 (None, None, None, 192) (None, None, None, 192)\n",
      "208 activation_537 (None, None, None, 192) (None, None, None, 192)\n",
      "209 conv2d_533 (None, None, None, 192) (None, None, None, 192)\n",
      "210 conv2d_538 (None, None, None, 192) (None, None, None, 192)\n",
      "211 batch_normalization_533 (None, None, None, 192) (None, None, None, 192)\n",
      "212 batch_normalization_538 (None, None, None, 192) (None, None, None, 192)\n",
      "213 activation_533 (None, None, None, 192) (None, None, None, 192)\n",
      "214 activation_538 (None, None, None, 192) (None, None, None, 192)\n",
      "215 average_pooling2d_52 (None, None, None, 768) (None, None, None, 768)\n",
      "216 conv2d_531 (None, None, None, 768) (None, None, None, 192)\n",
      "217 conv2d_534 (None, None, None, 192) (None, None, None, 192)\n",
      "218 conv2d_539 (None, None, None, 192) (None, None, None, 192)\n",
      "219 conv2d_540 (None, None, None, 768) (None, None, None, 192)\n",
      "220 batch_normalization_531 (None, None, None, 192) (None, None, None, 192)\n",
      "221 batch_normalization_534 (None, None, None, 192) (None, None, None, 192)\n",
      "222 batch_normalization_539 (None, None, None, 192) (None, None, None, 192)\n",
      "223 batch_normalization_540 (None, None, None, 192) (None, None, None, 192)\n",
      "224 activation_531 (None, None, None, 192) (None, None, None, 192)\n",
      "225 activation_534 (None, None, None, 192) (None, None, None, 192)\n",
      "226 activation_539 (None, None, None, 192) (None, None, None, 192)\n",
      "227 activation_540 (None, None, None, 192) (None, None, None, 192)\n",
      "228 mixed7 [(None, None, None, 192), (None, None, None, 192), (None, None, None, 192), (None, None, None, 192)] (None, None, None, 768)\n",
      "229 conv2d_543 (None, None, None, 768) (None, None, None, 192)\n",
      "230 batch_normalization_543 (None, None, None, 192) (None, None, None, 192)\n",
      "231 activation_543 (None, None, None, 192) (None, None, None, 192)\n",
      "232 conv2d_544 (None, None, None, 192) (None, None, None, 192)\n",
      "233 batch_normalization_544 (None, None, None, 192) (None, None, None, 192)\n",
      "234 activation_544 (None, None, None, 192) (None, None, None, 192)\n",
      "235 conv2d_541 (None, None, None, 768) (None, None, None, 192)\n",
      "236 conv2d_545 (None, None, None, 192) (None, None, None, 192)\n",
      "237 batch_normalization_541 (None, None, None, 192) (None, None, None, 192)\n",
      "238 batch_normalization_545 (None, None, None, 192) (None, None, None, 192)\n",
      "239 activation_541 (None, None, None, 192) (None, None, None, 192)\n",
      "240 activation_545 (None, None, None, 192) (None, None, None, 192)\n",
      "241 conv2d_542 (None, None, None, 192) (None, None, None, 320)\n",
      "242 conv2d_546 (None, None, None, 192) (None, None, None, 192)\n",
      "243 batch_normalization_542 (None, None, None, 320) (None, None, None, 320)\n",
      "244 batch_normalization_546 (None, None, None, 192) (None, None, None, 192)\n",
      "245 activation_542 (None, None, None, 320) (None, None, None, 320)\n",
      "246 activation_546 (None, None, None, 192) (None, None, None, 192)\n",
      "247 max_pooling2d_24 (None, None, None, 768) (None, None, None, 768)\n",
      "248 mixed8 [(None, None, None, 320), (None, None, None, 192), (None, None, None, 768)] (None, None, None, 1280)\n",
      "249 conv2d_551 (None, None, None, 1280) (None, None, None, 448)\n",
      "250 batch_normalization_551 (None, None, None, 448) (None, None, None, 448)\n",
      "251 activation_551 (None, None, None, 448) (None, None, None, 448)\n",
      "252 conv2d_548 (None, None, None, 1280) (None, None, None, 384)\n",
      "253 conv2d_552 (None, None, None, 448) (None, None, None, 384)\n",
      "254 batch_normalization_548 (None, None, None, 384) (None, None, None, 384)\n",
      "255 batch_normalization_552 (None, None, None, 384) (None, None, None, 384)\n",
      "256 activation_548 (None, None, None, 384) (None, None, None, 384)\n",
      "257 activation_552 (None, None, None, 384) (None, None, None, 384)\n",
      "258 conv2d_549 (None, None, None, 384) (None, None, None, 384)\n",
      "259 conv2d_550 (None, None, None, 384) (None, None, None, 384)\n",
      "260 conv2d_553 (None, None, None, 384) (None, None, None, 384)\n",
      "261 conv2d_554 (None, None, None, 384) (None, None, None, 384)\n",
      "262 average_pooling2d_53 (None, None, None, 1280) (None, None, None, 1280)\n",
      "263 conv2d_547 (None, None, None, 1280) (None, None, None, 320)\n",
      "264 batch_normalization_549 (None, None, None, 384) (None, None, None, 384)\n",
      "265 batch_normalization_550 (None, None, None, 384) (None, None, None, 384)\n",
      "266 batch_normalization_553 (None, None, None, 384) (None, None, None, 384)\n",
      "267 batch_normalization_554 (None, None, None, 384) (None, None, None, 384)\n",
      "268 conv2d_555 (None, None, None, 1280) (None, None, None, 192)\n",
      "269 batch_normalization_547 (None, None, None, 320) (None, None, None, 320)\n",
      "270 activation_549 (None, None, None, 384) (None, None, None, 384)\n",
      "271 activation_550 (None, None, None, 384) (None, None, None, 384)\n",
      "272 activation_553 (None, None, None, 384) (None, None, None, 384)\n",
      "273 activation_554 (None, None, None, 384) (None, None, None, 384)\n",
      "274 batch_normalization_555 (None, None, None, 192) (None, None, None, 192)\n",
      "275 activation_547 (None, None, None, 320) (None, None, None, 320)\n",
      "276 mixed9_0 [(None, None, None, 384), (None, None, None, 384)] (None, None, None, 768)\n",
      "277 concatenate_11 [(None, None, None, 384), (None, None, None, 384)] (None, None, None, 768)\n",
      "278 activation_555 (None, None, None, 192) (None, None, None, 192)\n",
      "279 mixed9 [(None, None, None, 320), (None, None, None, 768), (None, None, None, 768), (None, None, None, 192)] (None, None, None, 2048)\n",
      "280 conv2d_560 (None, None, None, 2048) (None, None, None, 448)\n",
      "281 batch_normalization_560 (None, None, None, 448) (None, None, None, 448)\n",
      "282 activation_560 (None, None, None, 448) (None, None, None, 448)\n",
      "283 conv2d_557 (None, None, None, 2048) (None, None, None, 384)\n",
      "284 conv2d_561 (None, None, None, 448) (None, None, None, 384)\n",
      "285 batch_normalization_557 (None, None, None, 384) (None, None, None, 384)\n",
      "286 batch_normalization_561 (None, None, None, 384) (None, None, None, 384)\n",
      "287 activation_557 (None, None, None, 384) (None, None, None, 384)\n",
      "288 activation_561 (None, None, None, 384) (None, None, None, 384)\n",
      "289 conv2d_558 (None, None, None, 384) (None, None, None, 384)\n",
      "290 conv2d_559 (None, None, None, 384) (None, None, None, 384)\n",
      "291 conv2d_562 (None, None, None, 384) (None, None, None, 384)\n",
      "292 conv2d_563 (None, None, None, 384) (None, None, None, 384)\n",
      "293 average_pooling2d_54 (None, None, None, 2048) (None, None, None, 2048)\n",
      "294 conv2d_556 (None, None, None, 2048) (None, None, None, 320)\n",
      "295 batch_normalization_558 (None, None, None, 384) (None, None, None, 384)\n",
      "296 batch_normalization_559 (None, None, None, 384) (None, None, None, 384)\n",
      "297 batch_normalization_562 (None, None, None, 384) (None, None, None, 384)\n",
      "298 batch_normalization_563 (None, None, None, 384) (None, None, None, 384)\n",
      "299 conv2d_564 (None, None, None, 2048) (None, None, None, 192)\n",
      "300 batch_normalization_556 (None, None, None, 320) (None, None, None, 320)\n",
      "301 activation_558 (None, None, None, 384) (None, None, None, 384)\n",
      "302 activation_559 (None, None, None, 384) (None, None, None, 384)\n",
      "303 activation_562 (None, None, None, 384) (None, None, None, 384)\n",
      "304 activation_563 (None, None, None, 384) (None, None, None, 384)\n",
      "305 batch_normalization_564 (None, None, None, 192) (None, None, None, 192)\n",
      "306 activation_556 (None, None, None, 320) (None, None, None, 320)\n",
      "307 mixed9_1 [(None, None, None, 384), (None, None, None, 384)] (None, None, None, 768)\n",
      "308 concatenate_12 [(None, None, None, 384), (None, None, None, 384)] (None, None, None, 768)\n",
      "309 activation_564 (None, None, None, 192) (None, None, None, 192)\n",
      "310 mixed10 [(None, None, None, 320), (None, None, None, 768), (None, None, None, 768), (None, None, None, 192)] (None, None, None, 2048)\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "     print (i, layer.name, layer.input_shape, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaurya\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)# let's add a fully-connected layer as first layer\n",
    "x = Dense(1024, activation='relu')(x)# and a logistic layer with 200 classes as last layer\n",
    "predictions = Dense(200, activation='softmax')(x)# model to train\n",
    "model = Model(input=base_model.input, output=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All the convolutional levels are pre-trained, so we freeze them during the training of the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# that is, freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers: layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# train the model on the new data for a few epochs model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then we freeze the top layers in inception and fine-tune some inception layer. In this example, we decide to freeze the first 172 layers (an hyperparameter to tune):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-55f1ebf6efe2>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-55f1ebf6efe2>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    for layer in:\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# we chose to train the top 2 inception blocks, that is, we will freeze\n",
    "\n",
    "# the first 172 layers and unfreeze the rest: \n",
    "for layer in: \n",
    "    model.layers[:172]: layer.trainable = False \n",
    "for layer in \n",
    "model.layers[172:]: layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model is then recompiled for fine-tune optimization. We need to recompile the model for these modifications to take effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we use SGD with a low learning rate\n",
    "#from keras.optimizers\n",
    "#import SGD\n",
    "\n",
    "#model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks)\n",
    "# alongside the top Dense layers\n",
    "\n",
    "#model.fit_generator(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study the effect of hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have studied the effect of different number of hidden units. I noticed that the increasing the number of hidden units improved the accuracy but there is a trade-off between the number of hidden units and computation cost and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 26,506.0\n",
      "Trainable params: 26,506.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.9008 - acc: 0.7044 - val_loss: 0.2933 - val_acc: 0.9186\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.4948 - acc: 0.8485 - val_loss: 0.2277 - val_acc: 0.9358\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.4190 - acc: 0.8752 - val_loss: 0.2028 - val_acc: 0.9425\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.3731 - acc: 0.8882 - val_loss: 0.1881 - val_acc: 0.9465\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.3536 - acc: 0.8950 - val_loss: 0.1795 - val_acc: 0.9503\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.3336 - acc: 0.9002 - val_loss: 0.1738 - val_acc: 0.9503\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.3235 - acc: 0.9026 - val_loss: 0.1699 - val_acc: 0.9523\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.3112 - acc: 0.9068 - val_loss: 0.1738 - val_acc: 0.9508\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.3058 - acc: 0.9081 - val_loss: 0.1663 - val_acc: 0.9543\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.2977 - acc: 0.9116 - val_loss: 0.1626 - val_acc: 0.9561\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2944 - acc: 0.9114 - val_loss: 0.1605 - val_acc: 0.9562\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2862 - acc: 0.9134 - val_loss: 0.1549 - val_acc: 0.9590\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2823 - acc: 0.9153 - val_loss: 0.1577 - val_acc: 0.9567\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2764 - acc: 0.9172 - val_loss: 0.1539 - val_acc: 0.9567\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2705 - acc: 0.9187 - val_loss: 0.1540 - val_acc: 0.9577\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2682 - acc: 0.9185 - val_loss: 0.1582 - val_acc: 0.9556\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2667 - acc: 0.9200 - val_loss: 0.1573 - val_acc: 0.9584\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2656 - acc: 0.9188 - val_loss: 0.1533 - val_acc: 0.9592\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2577 - acc: 0.9209 - val_loss: 0.1629 - val_acc: 0.9567\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s - loss: 0.2549 - acc: 0.9225 - val_loss: 0.1549 - val_acc: 0.9594\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "OPTIMIZER = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,batch_size=64, epochs=20,verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9568/10000 [===========================>..] - ETA:  - ETA: 0s\n",
      "Loss: 0.16, Accuracy: 95.68%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 55,050.0\n",
      "Trainable params: 55,050.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s - loss: 0.5869 - acc: 0.8194 - val_loss: 0.2106 - val_acc: 0.9370\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.3014 - acc: 0.9113 - val_loss: 0.1608 - val_acc: 0.9538\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.2537 - acc: 0.9250 - val_loss: 0.1452 - val_acc: 0.9585\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.2216 - acc: 0.9336 - val_loss: 0.1365 - val_acc: 0.9618\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.2014 - acc: 0.9397 - val_loss: 0.1211 - val_acc: 0.9647\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1898 - acc: 0.9439 - val_loss: 0.1187 - val_acc: 0.9673\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1804 - acc: 0.9466 - val_loss: 0.1160 - val_acc: 0.9659\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1689 - acc: 0.9486 - val_loss: 0.1089 - val_acc: 0.9683\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1606 - acc: 0.9525 - val_loss: 0.1036 - val_acc: 0.9694\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1552 - acc: 0.9528 - val_loss: 0.1073 - val_acc: 0.9683\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1512 - acc: 0.9542 - val_loss: 0.1056 - val_acc: 0.9683\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1503 - acc: 0.9545 - val_loss: 0.1034 - val_acc: 0.9704\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1379 - acc: 0.9577 - val_loss: 0.1017 - val_acc: 0.9699\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1345 - acc: 0.9583 - val_loss: 0.1028 - val_acc: 0.9697\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1344 - acc: 0.9590 - val_loss: 0.0996 - val_acc: 0.9710\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1277 - acc: 0.9599 - val_loss: 0.1011 - val_acc: 0.9713\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1228 - acc: 0.9618 - val_loss: 0.0971 - val_acc: 0.9725\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1269 - acc: 0.9610 - val_loss: 0.1011 - val_acc: 0.9718\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1203 - acc: 0.9632 - val_loss: 0.0996 - val_acc: 0.9698\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s - loss: 0.1214 - acc: 0.9616 - val_loss: 0.0997 - val_acc: 0.9711\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(64, input_shape=(784,)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(64))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(10))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "\n",
    "OPTIMIZER = Adam()\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history2 = model2.fit(X_train, Y_train,batch_size=64, epochs=20,verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9664/10000 [===========================>..] - ETA:  - ETA: 0s\n",
      "Loss: 0.09, Accuracy: 97.25%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "loss, accuracy = model2.evaluate(X_test, Y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282.0\n",
      "Trainable params: 118,282.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s - loss: 0.4461 - acc: 0.8652 - val_loss: 0.1682 - val_acc: 0.9501\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.2112 - acc: 0.9367 - val_loss: 0.1241 - val_acc: 0.9633\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.1642 - acc: 0.9507 - val_loss: 0.1103 - val_acc: 0.9683\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.1376 - acc: 0.9588 - val_loss: 0.1012 - val_acc: 0.9700\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.1212 - acc: 0.9626 - val_loss: 0.0983 - val_acc: 0.9693\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.1090 - acc: 0.9662 - val_loss: 0.0959 - val_acc: 0.9744\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.1003 - acc: 0.9686 - val_loss: 0.0918 - val_acc: 0.9728\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0932 - acc: 0.9708 - val_loss: 0.0885 - val_acc: 0.9732\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0819 - acc: 0.9750 - val_loss: 0.0905 - val_acc: 0.9739\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0799 - acc: 0.9743 - val_loss: 0.0878 - val_acc: 0.9744\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0784 - acc: 0.9761 - val_loss: 0.0925 - val_acc: 0.9737\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0728 - acc: 0.9770 - val_loss: 0.0848 - val_acc: 0.9765\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0712 - acc: 0.9778 - val_loss: 0.0806 - val_acc: 0.9780\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0662 - acc: 0.9788 - val_loss: 0.0838 - val_acc: 0.9769\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0652 - acc: 0.9790 - val_loss: 0.0852 - val_acc: 0.9763\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0619 - acc: 0.9801 - val_loss: 0.0818 - val_acc: 0.9775\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0610 - acc: 0.9806 - val_loss: 0.0823 - val_acc: 0.9777\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0564 - acc: 0.9823 - val_loss: 0.0890 - val_acc: 0.9773\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0542 - acc: 0.9823 - val_loss: 0.0861 - val_acc: 0.9787\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s - loss: 0.0560 - acc: 0.9821 - val_loss: 0.0809 - val_acc: 0.9781\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(128, input_shape=(784,)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(128))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(10))\n",
    "model3.add(Activation('softmax'))\n",
    "model3.summary()\n",
    "\n",
    "OPTIMIZER = Adam()\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history3 = model3.fit(X_train, Y_train,batch_size=64, epochs=20,verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9216/10000 [==========================>...] - ETA: 0s\n",
      "Loss: 0.08, Accuracy: 97.82%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "loss, accuracy = model3.evaluate(X_test, Y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 669,706.0\n",
      "Trainable params: 669,706.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.2675 - acc: 0.9187 - val_loss: 0.1199 - val_acc: 0.9627\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.1225 - acc: 0.9620 - val_loss: 0.1031 - val_acc: 0.9692\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0924 - acc: 0.9723 - val_loss: 0.0898 - val_acc: 0.9738\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0751 - acc: 0.9759 - val_loss: 0.0827 - val_acc: 0.9749\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0662 - acc: 0.9789 - val_loss: 0.0894 - val_acc: 0.9758\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0599 - acc: 0.9807 - val_loss: 0.0742 - val_acc: 0.9794\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0486 - acc: 0.9850 - val_loss: 0.0737 - val_acc: 0.9798\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0505 - acc: 0.9842 - val_loss: 0.0744 - val_acc: 0.9808\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0425 - acc: 0.9860 - val_loss: 0.0853 - val_acc: 0.9781\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0413 - acc: 0.9861 - val_loss: 0.0899 - val_acc: 0.9785\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0392 - acc: 0.9875 - val_loss: 0.0808 - val_acc: 0.9808\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0354 - acc: 0.9888 - val_loss: 0.0798 - val_acc: 0.9816\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0332 - acc: 0.9886 - val_loss: 0.0921 - val_acc: 0.9792\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0310 - acc: 0.9902 - val_loss: 0.0926 - val_acc: 0.9781\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0322 - acc: 0.9896 - val_loss: 0.0943 - val_acc: 0.9777\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0289 - acc: 0.9909 - val_loss: 0.1045 - val_acc: 0.9788\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 25s - loss: 0.0325 - acc: 0.9897 - val_loss: 0.0913 - val_acc: 0.9818\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 24s - loss: 0.0288 - acc: 0.9913 - val_loss: 0.0958 - val_acc: 0.9801\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0273 - acc: 0.9919 - val_loss: 0.0921 - val_acc: 0.9802\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 23s - loss: 0.0281 - acc: 0.9911 - val_loss: 0.0863 - val_acc: 0.9809\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(512, input_shape=(784,)))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dropout(0.3))\n",
    "model4.add(Dense(512))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dropout(0.3))\n",
    "model4.add(Dense(10))\n",
    "model4.add(Activation('softmax'))\n",
    "model4.summary()\n",
    "\n",
    "OPTIMIZER = Adam()\n",
    "model4.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history4 = model4.fit(X_train, Y_train,batch_size=64, epochs=20,verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9664/10000 [===========================>..] - ETA: 0s\n",
      "Loss: 0.07, Accuracy: 98.37%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "loss, accuracy = model4.evaluate(X_test, Y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                10250     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,863,690.0\n",
      "Trainable params: 1,863,690.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 64s - loss: 0.2384 - acc: 0.9273 - val_loss: 0.1197 - val_acc: 0.9630\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 63s - loss: 0.1126 - acc: 0.9654 - val_loss: 0.1196 - val_acc: 0.9643\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 62s - loss: 0.0867 - acc: 0.9723 - val_loss: 0.0960 - val_acc: 0.9694\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 62s - loss: 0.0703 - acc: 0.9776 - val_loss: 0.0896 - val_acc: 0.9754\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 63s - loss: 0.0605 - acc: 0.9810 - val_loss: 0.0848 - val_acc: 0.9763\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 64s - loss: 0.0566 - acc: 0.9825 - val_loss: 0.0799 - val_acc: 0.9771\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 62s - loss: 0.0506 - acc: 0.9844 - val_loss: 0.0987 - val_acc: 0.9744\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 62s - loss: 0.0449 - acc: 0.9858 - val_loss: 0.1225 - val_acc: 0.9708\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 62s - loss: 0.0418 - acc: 0.9867 - val_loss: 0.0898 - val_acc: 0.9777\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 62s - loss: 0.0376 - acc: 0.9878 - val_loss: 0.0936 - val_acc: 0.9793\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 61s - loss: 0.0392 - acc: 0.9879 - val_loss: 0.1042 - val_acc: 0.9762\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 62s - loss: 0.0352 - acc: 0.9893 - val_loss: 0.1007 - val_acc: 0.9786\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 61s - loss: 0.0294 - acc: 0.9909 - val_loss: 0.1118 - val_acc: 0.9782\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 61s - loss: 0.0351 - acc: 0.9897 - val_loss: 0.0978 - val_acc: 0.9792\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 61s - loss: 0.0350 - acc: 0.9900 - val_loss: 0.0928 - val_acc: 0.9812\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 61s - loss: 0.0309 - acc: 0.9912 - val_loss: 0.0989 - val_acc: 0.9794\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 61s - loss: 0.0346 - acc: 0.9899 - val_loss: 0.0978 - val_acc: 0.9804\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 61s - loss: 0.0247 - acc: 0.9922 - val_loss: 0.0998 - val_acc: 0.9789\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 61s - loss: 0.0307 - acc: 0.9913 - val_loss: 0.1119 - val_acc: 0.9802\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 60s - loss: 0.0271 - acc: 0.9921 - val_loss: 0.1207 - val_acc: 0.9786\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Dense(1024, input_shape=(784,)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.3))\n",
    "model5.add(Dense(1024))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.3))\n",
    "model5.add(Dense(10))\n",
    "model5.add(Activation('softmax'))\n",
    "model5.summary()\n",
    "\n",
    "OPTIMIZER = Adam()\n",
    "model5.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history5 = model5.fit(X_train, Y_train,batch_size=64, epochs=20,verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s     \n",
      "\n",
      "Loss: 0.10, Accuracy: 98.06%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "loss, accuracy = model5.evaluate(X_test, Y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.95679999999999998,\n",
       " 0.97250000000000003,\n",
       " 0.97819999999999996,\n",
       " 0.98370000000000002,\n",
       " 0.98060000000000003]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_units=[32,64,128,512,1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8XWWd7/HPt0nTNG16S9rS+4VWoEALWBFUKOINlCqi\nhwEvI97wePTIqHjhOOOF0ePMGUdHHY4jOiioR1RwlHJH5CIqSBHaAqVQ2kLbpKVJr0ma++/8sdZO\nd9O03Wmyu5Ps7/v12q/svZ61Vp6V1eaX53nW83sUEZiZmR2pYYWugJmZDW4OJGZm1icOJGZm1icO\nJGZm1icOJGZm1icOJGZm1icOJFYwkjZIev1Bys6StOYQx/5Y0lcPUR6S5vVHPfuLpP8l6YeFrodZ\nfystdAXMehIRfwCOK3Q9+lNE/O/Me0mzgfXA8IhoL1SdzPqDWyRm1iuS/Aeo7ceBxArtFEkrJe2S\n9AtJ5QCSzpG0KbOTpFMl/VXSHkm/AMqzTyLpM5JqJdVI+kC3shGSviHpRUlbJf2HpJHZ30fSpyW9\nlJ7j/QerbPfuOElflvTT9P3stEvtfen3qpP0hZ72BR5Mv+6U1CDpTEnzJD2Q/izq0us8WD1+JWlL\nuu+Dkk7MKhsp6V8lvZCWP5R1va+R9CdJOyVtlHRZuv1+SR/KOsdlkh7K+hySPibpOeC5dNu303Ps\nlvSYpLOy9i9Ju/KeT+/ZY5JmSLpG0r92u5ZbJH3yYNdqA58DiRXaxcB5wBxgIXBZ9x0klQG/AX4C\nTAB+Bbwjq/w84ErgDcB8oPu4yz8BLwNOAeYB04AvZpUfA4xNt38QuEbS+D5c02tIuuVeB3xR0gk9\n7HN2+nVcRIyOiD8D/wjcDYwHpgPfPcT3uIPkWicBfwV+llX2DeDlwKtIfl6fBTolzUqP+y4wkeTn\n8UQvrutC4JXAgvTzo+k5JgD/D/hV5g8B4FPApcCbgTHAB4Am4HrgUknDACRVk9yv/9eLetgA40Bi\nhfadiKiJiO3AMpJfTN2dAQwH/i0i2iLiJpJfYhkXAz+KiCcjohH4cqZAkoDLgU9GxPaI2AP8b+CS\nrOPbgKvTc98ONNC38ZmvRMTeiFgBrAAW5XhcGzALmBoRzRHx0MF2jIjrImJPRLSQXO8iSWPTX9Af\nAK6IiM0R0RERf0r3exfwu4j4eXqt9RHRm0Dy9fRnuDetw0/Tc7RHxL8CI9j3c/sQ8PcRsSYSK9J9\n/wLsIgmykNyH+yNiay/qYQOMA4kV2pas903A6B72mQpsjv0zjL7QrXzjQcomAhXAY2l3zk7gznR7\nRn23Ae+D1SNXuVxTTz4LCPiLpKe6d9FlpN1G/5R2G+0GNqRF1emrHHi+h0NnHGR7rrJ/xki6UtLq\ntPtsJ0mrrjqH73U98J70/XtIWpo2iDmQ2GBQC0xLWxcZM7uVzzhIWR2wFzgxIsalr7ERcaSBopEk\nMGUcc4TnOSDtdkRsiYgPR8RU4CPA/z3II8zvAt5G0iU0FpidbhfJ9TYDx/Zw3MaDbIfcrqurzul4\nyGdJWoPjI2IcSUsjc48O9b1+CrxN0iLgBJJuSxvEHEhsMPgz0A58QtJwSRcBp2eV/xK4TNICSRXA\nlzIFEdEJ/AD4lqRJAJKmSXrTEdblCeCStB6LgXce4Xm2AZ3A3MwGSf9N0vT04w6SX9ydPRxbCbQA\n9SS//LseK06v9zrgm5Kmpq2XMyWNIBlHeb2kiyWVSqqSlOlKfAK4SFJFGrw+eJj6V5Lck21AqaQv\nkoyFZPwQ+EdJ85VYKKkqreMmkq7JnwA3Z7rKbPByILEBLyJagYtIBuK3A38D/Dqr/A7g34DfA2vT\nr9k+l25/OO0K+h1HPgbyDyR/ae8AvsIRDhJHRBPwNeCPaZfbGcArgEckNQC3kIxzrOvh8BtIuu82\nA08DD3crvxJYRfLLejvwz8CwiHiRZPD70+n2J9g3fvMtoBXYStL19DMO7S6SLsJn07o0s3/X1zdJ\nAvzdwG7gP4GRWeXXAyfjbq0hQV7YysyONklnk3RxzQr/Ehr03CIxs6NK0nDgCuCHDiJDgwOJmR01\n6ZyancAUku5IGwLyGkgknSdpjaS1kj7fQ/ksSfcqmdl8f9ZAI5L+T/oI5GpJ38k8sZPut0bSE+lr\nUj6vwcz6T0SsjohREfGqiNhd6PpY/8hbIJFUAlwDnE8yE/ZSSQu67fYN4IaIWAhcDXw9PfZVwKtJ\nZjqfRDIIuSTruHdHxCnp66V8XYOZmR1ePpOvnQ6szTx1IulGkmffn87aZwFJKgWA+9j3PHmQTKoq\nI3kufTjJ0yRHpLq6OmbPnn2kh5uZFaXHHnusLiImHm6/fAaSaez/OOAmkjw92VaQPNb5beDtQKWk\nqoj4s6T7SCaaCfj3iFidddyPJHUANwNf7WnATtLlJKkxmDlzJsuXL++nyzIzKw6SXjj8XoUfbL8S\nWCLpcZKuq81ARzoh6gSSxHXTgHOzMou+OyJOBs5KX+/t6cQRcW1ELI6IxRMnHjagmpnZEcpnINnM\n/mkrpqfbuqTJ+i6KiFOBL6TbdpK0Th6OiIaIaCDJWHpmWr45/bqHZDJY9gxnMzM7yvIZSB4F5kua\nk6YBv4Rktm4XSdWZdNLAVSSpHQBeJGmplKbPnC8BVqefq9NjhwMXAE/m8RrMzOww8hZI0myqHydJ\npbAa+GVEPCXpaklvTXc7B1gj6VlgMknKCICbSDKHriIZR1kREctI0lTfJWklSXqHzSR5lMzMrECK\nIkXK4sWLw4PtZma9I+mxiFh8uP0KPdhuZmaDnAOJmZn1ST7nkZgZ0N7RyZ/X1bNq8y6mjRvJnOpR\nzK4exZjy4YWumlm/cCAxy4POzuDRDdtZtrKGO1Ztob6x9YB9qkeXMbsqCSpzqkel7yuYXTWKUSP8\nX9MGD/9rNesnEcGKTbtYtqKG21bWsmV3M+XDh/H6EyazdNFUzphTxZbdzayva2R9XSMb6hpZX9/I\ng89u46bHNu13rkmVI5iTCTBpkJlTPYpZVRWUDy8p0BWa9cyBxKwPIoLVtXtYtrKGW1fWsHH7XspK\nhrHkuIn8r0Un8LrjJ+3XuhhbMZzjjqk84DyNLe1sqG9kQ10TG+obWbetkQ31jdzz9Nb9WjMSTBlT\n3tWK2deSGcXMCRWUlXrY044+BxKzI7D2pQZuXVnDshU1PL+tkZJh4jXzqrnidS/jDQsmM3Zk78Y/\nRo0o5cSpYzlx6tgDynY3tyWtl7qsQFPXyK0ra9m1t61rv2GCaeNHdrVeMq2ZOVWjmD5+JKUlDjKW\nHw4kZjnauL2JZStrWLailtW1u5HgjDlVfOA1czj/pClMGFWWl+87pnw4C6ePY+H0cQeU7WhsZX19\n0k2WdJU1saGukV//dTMNLe1d+5UOEzMmVDC7quKA1szUcSMpGaa81N2KgwOJ2SFs2dXMbatqWbai\nhic27gTgtJnj+NLSBbz55ClMHlNe0PqNH1XG+FFlnDZz/H7bI4K6hlY21O8bj0neN/Hwuu3sbevo\n2resZBgzqyrSlsz+gWZyZTnDHGTsMBxIzLqpa2jhjie3sGxFDY9u2E4EnDRtDJ8//3jecvIUZkyo\nKHQVD0sSEytHMLFyBK+YPWG/sohg6+6WJMCkrZnM+wef20Zre2fXvuXDhyVjMF1Pl6UBZ+IoJo4e\nQbpwqRU5BxIzYFdTG3c9tYVlK2v40/P1dHQG8yaN5pOvfxkXLJzC3ImjC13FfiOJY8aWc8zYcs48\ntmq/ss7OoGbXXjbUNe3XZfbsS3u495mttHXsS6k0qqwkeaIsHYfJDjQTRpU5yBQRBxIrWo0t7fxu\n9VaWrajhgWe30dYRzKqq4KNLjuWCRVM4bnJl0f0yHDZMTB9fwfTxFbxmfvV+Ze0dndTsbGZ9fSPr\ntzWwob6J9XWNPLl5F3c+uYWOzn1BprK8lLndHl3OBJyxFZ6IOdQ4aaMVlea2Du575iWWrazh98+8\nRHNbJ1PGlnPBwiksXTSVk6eNLbrg0R9a2zvZtKOpaxxmfV1D0qqpa6Rm116yf82Mrxi+X2CZnfWE\n2WhPxBxQck3a6LtmQ15reycPrd3GshW13P3UFhpbO6geXcbfLJ7BBYum8vKZ4z2g3EdlpcOYO3F0\nj12AzW0dbNze1DUOkwk0f1pbz6//ut9ad1SPHsGc6ooDAs3sqlGMLPNEzIHKgcSGpPaOTh5et51l\nK2q486kt7NrbxtiRw1m6aCpLF03llXMmeF7FUVI+vIT5kyuZP/nAiZh7Wzv2Dfh3jck08ftntlHX\nsP9s/2PGlDM7DTJzsrrMZkzwbP9CcyCxIaOzM1j+wg5uXVnD7atqqWtoZfSIUt64IElR8up51Z75\nPcCMLCvhhCljOGHKmAPK9jS38UI6DpMdaO56aivbu832nzo2kwyzgjnVo7sG/WdMqGC4/2DIOwcS\nG9QigpVpfqtbs/Jbve74ySxdNIVzjpvkv1YHqcry4Zw0bSwnTTtwtv+uprauwJL9GPMtT9Swu3nf\nRMySYWJ61mz/zITMudWjmTqu3K3SfuJAYoNORPDMlj1pipJaXtzexPASseRlk7jqzcfz+hMmO3vu\nEDe2YjinVIzjlBn7z/aPCLY3tnaNxWS3ZJZv2E5j676JmMNLktn+XeMwXWMyFUwdO9LjZr3g/202\naDy/rYFbV9SybGUNa19qoGSYeNWxVXz83Hm8acExfqzUkETV6BFUjR7By2cdOBFzW0NL+jRZQ1eg\n2VDfyB+fr6O5bd9EzBGlw5jVNdt//8eYJ4/xRMzuHEhsQNu4vYlbV9Zy68oanqpJ8ludPnsCl114\nEuefdAxVo0cUuoo2SEhiUmU5kyrLOX3O/kGmszPYuqe5KzFmJtCsq2vk/jXbaO3YF2QqykqYVbVv\n8mX2ejLVo4tzIqYDiQ04W3c3c9vKpOXx+ItJfqtTZozjHy5YwFtOnsIxYwub38qGnmHDxJSxI5ky\ndiSvOnb/so7OoGbn3qx0MkmgWV27h7uf2kp79kTMEaVZ3WQV+3WZjc9TUs+BwBMSbUCoz8pv9Zc0\nv9WCKWNYumgqFywcHPmtrPi0d3SyacfeAzIwr69rYPOOvWTFGMaOHJ4O9O9bDXOgL7vsCYk24O3a\n28bdT21h2cpa/ri2jo7O4NiJo7jidfO5YOFU5k0aOvmtbGgqLRnW1erguP3LWto72Lh9b1bm5eTr\nX9Zv578e338iZtWosq5xmLkTB9+yywO/hjak7MtvVcuDzyZ9zzMmjOQjZ89l6aKpHH9M8eW3sqFp\nRGkJ8yaN7vEPoua2jn1zZLIeY35o7TZu/uuByy53Tycz0JZddiCxvGtu6+D+NS+xbGUt967eSnNb\nJ8eMKee9Z85i6aKpLJru/FZWXMqHl3DcMZU5LbucmZB57zNbqWto3W/fqWPLe8jAfPSXXXYgsbxo\nbe/kj2vrWLaihruf3kpDSztVo8r4by+fwdJFU1k8y/mtzHpyuGWXX6hrYl2aFDMTaG5fVcvOpp6X\nXf7qhScxq2pUXuvsQGL9pqMzeGRdPctW1nDHk1vY2dTGmPJS3nLyFC5YNIUz51Z5JrFZH4wpH87J\n08dy8vQDg8zOptZ9iTG37Vt2+Wgku8xrIJF0HvBtoAT4YUT8U7fyWcB1wERgO/CeiNiUlv0f4C3A\nMOAe4IqICEkvB34MjARuz2zP53XYwXV2Bn99cQfLVtRw26ot1DW0MKqshDek+a3Omj/R+a3MjoJx\nFWWcOrOMU7stu3w05C2QSCoBrgHeAGwCHpV0S0Q8nbXbN4AbIuJ6SecCXwfeK+lVwKuBhel+DwFL\ngPuB7wEfBh4hCSTnAXfk6zrsQBHBqs1JfqvbVtZSs6uZEaXDOPf4SSxdNJXXHjfJKb/Nikg+WySn\nA2sjYh2ApBuBtwHZgWQB8Kn0/X3Ab9L3AZQDZYCA4cBWSVOAMRHxcHrOG4ALcSA5KtZs2cOyFTUs\nW1nDC/VJfquz50/ks+cdz+sXTPaiRGZFKp//86cBG7M+bwJe2W2fFcBFJN1fbwcqJVVFxJ8l3QfU\nkgSSf4+I1ZIWp+fJPue0nr65pMuBywFmzpzZD5dTnNZta+hKUfLs1gaGCV49r5qPnTOPN53o/FZm\nVvjB9iuBf5d0GfAgsBnokDQPOAGYnu53j6SzgL25njgirgWuhWRme39WeqjbtKOpK0XJk5t3A0l+\nq39824mcd9IUJlY6v5WZ7ZPPQLIZmJH1eXq6rUtE1JC0SJA0GnhHROyU9GHg4YhoSMvuAM4EfsK+\n4NLjOe3IvLS7mdtW1bJsRQ1/TfNbLZoxjr9/ywm8ZeEUpowdWeAamtlAlc9A8igwX9Ickl/2lwDv\nyt5BUjWwPSI6gatInuACeBH4sKSvk3RtLQH+LSJqJe2WdAbJYPvfAt/N4zUMadsbW7kzzW/18Pp6\nIuD4Yyr5zJuOY+nCqcyscn4rMzu8vAWSiGiX9HHgLpLHf6+LiKckXQ0sj4hbgHOAr0sKkq6tj6WH\n3wScC6wiGXi/MyKWpWX/g32P/96BB9p7ZXdzG3c/tZVlK2r449o62juDudWj+MS581m6aArzJh04\n09bM7FCc/bcINLW2c+/ql1i2oqZrbYVp40aydNFUli6awoIpY5yixMwO4Oy/Ra65rYMHnt3GshU1\n3Lv6Jfa2dTCpcgTvOWMWFyyawqkzxjl4mFm/cCAZQto6OnlobR23rqjl7qe2sKelnQmjyrjotGks\nXTSVV8yeQInzW5lZP3MgGeQ6OoNH1tezbEUtdz5Zy46mNirLSznvpGNYumgqrzrW+a3MLL8cSAah\nzs7g8Y07WLailttW1bJtTwsVZSW8/oQkv9XZL6tmRKlTlJjZ0eFAMkhEBE/V7GbZihpuXVnL5p17\nKSsdxrnHJfmtzj3e+a3MrDAcSAa4Z7fu6Qoe6+saKR0mzppfzaff+DLesGAylQN0rWczKx4OJAPQ\nC/WNSXLEFbWs2bqHYYIzj63iI2fP5U0nHsP4UWWFrqKZWRcHkgHmL+u3c+kPHqajM1g8azxfeeuJ\nnH/yMUyqLC901czMeuRAMsB89/fPMWFUGb/52KuZNs75rcxs4PNzoQPIqk27+MNzdXzwNXMcRMxs\n0HAgGUD+44HnqSwv5d2v9PopZjZ4OJAMEOvrGrn9yVree8YsP4llZoOKA8kAce2DzzO8ZBjvf/Wc\nQlfFzKxXHEgGgK27m7n5sc1cvHi6Vx80s0HHgWQAuO6h9bR3dnL5WccWuipmZr3mQFJgu5ra+OnD\nL3CBVyQ0s0HKgaTAfvrICzS2dvDfl7g1YmaDkwNJATW3dXDdQ+s557iJLJg6ptDVMTM7Ig4kBfSr\n5Rupb2zlo26NmNkg5kBSIO0dnXz/wXWcNnMcp8+ZUOjqmJkdMQeSArltVS2bduzlo+fM89rpZjao\nOZAUQETwvfufZ/6k0bzu+EmFro6ZWZ84kBTA/Wu28cyWPfz3JccybJhbI2Y2uDmQFMD37n+eqWPL\neespUwtdFTOzPnMgOcqWb9jOXzZs58Nnz2V4iX/8Zjb4+TfZUfYfDzzP+Irh/M0rZhS6KmZm/cKB\n5Chas2UPv1v9Epe9ag4VZV6c0syGBgeSo+j7DzxPRVkJf3vmrEJXxcys3+Q1kEg6T9IaSWslfb6H\n8lmS7pW0UtL9kqan218r6YmsV7OkC9OyH0tan1V2Sj6vob9s2tHEb1fUcOnpMxk/qqzQ1TEz6zd5\n61+RVAJcA7wB2AQ8KumWiHg6a7dvADdExPWSzgW+Drw3Iu4DTknPMwFYC9ydddxnIuKmfNU9H374\nh/UME3zoLC9cZWZDSz5bJKcDayNiXUS0AjcCb+u2zwLg9+n7+3ooB3gncEdENOWtpnlW39DCjY++\nyIWnTGPK2JGFro6ZWb/KZyCZBmzM+rwp3ZZtBXBR+v7tQKWkqm77XAL8vNu2r6XdYd+S1OOSgpIu\nl7Rc0vJt27Yd2RX0k+v/tIGW9k4+smRuQethZpYPhR5svxJYIulxYAmwGejIFEqaApwM3JV1zFXA\n8cArgAnA53o6cURcGxGLI2LxxIkT81T9w2toaef6P7/AGxdMZt6kyoLVw8wsX/L5DOpmIHuyxPR0\nW5eIqCFtkUgaDbwjInZm7XIx8F8R0ZZ1TG36tkXSj0iC0YB1419eZNfeNi9cZWZDVj5bJI8C8yXN\nkVRG0kV1S/YOkqolZepwFXBdt3NcSrdurbSVgpKUuRcCT+ah7v2ipb2DH/xhHWfOreLUmeMLXR0z\ns7zIWyCJiHbg4yTdUquBX0bEU5KulvTWdLdzgDWSngUmA1/LHC9pNkmL5oFup/6ZpFXAKqAa+Gq+\nrqGvfvt4DVt3t/DRc9waMbOhSxFR6Drk3eLFi2P58uVH9Xt2dAZv+NYDjBxewq3/8zVec8TMBh1J\nj0XE4sPtV+jB9iHrnqe3sG5bIx8951gHETMb0hxI8iCzcNWsqgrOP2lKoatjZpZXDiR58Ofn61mx\naRcfOftYSrxwlZkNcQ4kefC9B55nYuUILjqt+/xLM7Ohx4Gkn63atIs/PFfHB18zh/LhJYWujplZ\n3jmQ9LP/eOB5KstLefcrZxa6KmZmR4UDST9aX9fI7U/W8t4zZlFZPrzQ1TEzOyocSPrRtQ8+z/CS\nYbz/1U4Vb2bFw4Gkn2zd3czNj23m4sXTmVjZY0JiM7MhKadAIunXkt6SlRfLurnuofW0d3Zy+VlO\nh2JmxSXXwPB/gXcBz0n6J0nH5bFOg86upjZ++vALXLBwKjOrKgpdHTOzoyqnQBIRv4uIdwOnARuA\n30n6k6T3Syr6UeWfPvICja0dThVvZkUp566qdOXCy4APAY8D3yYJLPfkpWaDyC8e3chZ86tZMHVM\noatiZnbU5bSwlaT/Ao4DfgIszVpc6heSjm5a3QEmIqjdtZc3n+ycWmZWnHJdIfE7EXFfTwW5pBge\nynY3t9PWEVSPLit0VczMCiLXrq0FksZlPkgaL+l/5KlOg8r2xlYAJoxyIDGz4pRrIPlw9lrqEbED\n+HB+qjS41De0AFA12nNHzKw45RpISpS1OpOkEsB/ggN1DUmLpMotEjMrUrmOkdxJMrD+/fTzR9Jt\nRa++MWmRVLtFYmZFKtdA8jmS4PHR9PM9wA/zUqNBZnuDx0jMrLjlFEgiohP4XvqyLPWNrVSWl1JW\n6uwxZlaccp1HMh/4OrAAKM9sj4i5earXoFHX0OJuLTMrarn+Gf0jktZIO/Ba4AaSyYlFr76h1QPt\nZlbUcg0kIyPiXkAR8UJEfBk4N3/VGjzqG1uo8mREMytiuQaSljSF/HOSPi7p7cCkPNZr0Nje2Oo5\nJGZW1HINJFcAFcAngJcD7wHel69KDRYdnZEEEndtmVkRO+xgezr58OKI+AzQALw/77UaJHY2tdIZ\nnoxoZsXtsC2SiOgAXp49s90S9WmeLXdtmVkxy7Vr63Hgt5LeK+mizOtwB0k6T9IaSWslfb6H8lmS\n7pW0UtL9kqan218r6YmsV7OkC9OyOZIeSc/5C0kFaw7UdeXZcovEzIpXroFkAlBP8qTW0vR1waEO\nSLvErgHOJ5l/cqmkBd12+wZwQ0QsBK4mmatCRNwXEadExCnp92wC7k6P+WfgWxExD9gBfDDHa+h3\nmcy/nkdiZsUs15ntRzIucjqwNiLWAUi6EXgb8HTWPguAT6Xv7wN+08N53gncERFNaffauSTrxwNc\nD3yZAs24r3d6FDOznGe2/wiI7tsj4gOHOGwasDHr8ybgld32WQFcRLJs79uBSklVEVGftc8lwDfT\n91XAzohozzrntIPU+XLgcoCZM2ceoppHrr6hBQnGVziQmFnxyrVr61bgtvR1LzCG5AmuvroSWCLp\ncWAJsBnoyBRKmgKcDNzV2xNHxLURsTgiFk+cOLEfqnqgusZWJlSUUTLMzyGYWfHKtWvr5uzPkn5O\nkgH4UDYDM7I+T0+3ZZ+3hqRFgqTRwDuyF9ACLgb+KyLa0s/1wDhJpWmr5IBzHk31DZ7VbmZ2pClr\n5wOzDrPPo8D89CmrMpIuqluyd5BUnc6YB7gKuK7bOS4Ffp75EBFBMpbyznTT+4DfHtEV9IPtja0e\nHzGzopdTIJG0R9LuzAtYRrJGyUGlLYaPk3RLrQZ+GRFPSbpa0lvT3c4B1kh6FpgMfC3re84madE8\n0O3UnwM+JWktyZjJf+ZyDflQ3+D0KGZmuXZtVR7JySPiduD2btu+mPX+JuCmgxy7gR4G0tOnwE4/\nkvr0t7qGFqrdIjGzIpdri+TtksZmfR6XmSBYrFrbO9nd3O4WiZkVvVzHSL4UEbsyH9IB8S/lp0qD\nw46mTHoUt0jMrLjlGkh62i/X9d6HpK70KO7aMrMil2sgWS7pm5KOTV/fBB7LZ8UGusysdndtmVmx\nyzWQ/E+gFfgFcCPQDHwsX5UaDOob3SIxM4Pcn9pqBA7I3lvM3CIxM0vk+tTWPZLGZX0eL6nXaUuG\nkvrGVoaXiDHlRT1UZGaWc9dWdXbqkojYQZGv2V7f0MKEUWV4vS8zK3a5BpJOSV0pdNNZ5wdkAy4m\n9Q2tVI1yt5aZWa79Ml8AHpL0ACDgLNIU7cWqrrHVc0jMzMixRRIRdwKLgTUkSRQ/DezNY70GvPqG\nFq+MaGZG7gtbfQi4giRt+xPAGcCfSVYrLErO/Gtmlsh1jOQK4BXACxHxWuBUYFveajXANbW209Ta\n4a4tMzNyDyTNEdEMIGlERDwDHJe/ag1smTkk1R5sNzPLebB9UzqP5DfAPZJ2ADX5q9bAVt/ohI1m\nZhm5zmx/e/r2y5LuA8YCd+atVgPc9kx6FA+2m5n1PoNvRHRfsbDo1GXSo3iw3czsiNdsL2r78mw5\nkJiZOZAcgfqGFkYOL6GizHm2zMwcSI5AvWe1m5l1cSA5Akkg8UC7mRk4kByR+oYWD7SbmaUcSI5A\nkvnXgcTMDBxIei0iqG9scdeWmVnKgaSXdje309YRVHuw3cwMcCDpte1pehRn/jUzSziQ9FJ9g9Oj\nmJlly2sgkXSepDWS1kr6fA/lsyTdK2mlpPslTc8qmynpbkmrJT2dLu+LpB9LWi/pifR1Sj6voTun\nRzEz218PoK24AAANTUlEQVTeAomkEuAa4HxgAXCppAXddvsGcENELASuBr6eVXYD8C8RcQJwOvBS\nVtlnIuKU9PVEvq6hJ/Vpwkavjmhmlshni+R0YG1ErIuIVuBG4G3d9lkA/D59f1+mPA04pRFxD0BE\nNEREUx7rmrNMni2PkZiZJfIZSKYBG7M+b0q3ZVsBXJS+fztQKakKeBmwU9KvJT0u6V/SFk7G19Lu\nsG9JOqpNg+2NrVSWl1JW6uElMzMo/GD7lcASSY8DS4DNQAdJevuz0vJXAHOBy9JjrgKOT7dPAD7X\n04klXS5puaTl27b136rAdQ0t7tYyM8uSz0CyGZiR9Xl6uq1LRNRExEURcSrwhXTbTpLWyxNpt1g7\nycqMp6XltZFoAX5E0oV2gIi4NiIWR8TiiRMn9ttFeVa7mdn+8hlIHgXmS5ojqQy4BLglewdJ1ZIy\ndbgKuC7r2HGSMhHgXODp9Jgp6VcBFwJP5vEaDpDMancgMTPLyFsgSVsSHwfuAlYDv4yIpyRdLemt\n6W7nAGskPQtMBr6WHttB0q11r6RVgIAfpMf8LN22CqgGvpqva+jJdmf+NTPbT15XZoqI24Hbu237\nYtb7m4CbDnLsPcDCHraf28/VzFlHZySBxF1bZmZdCj3YPqjsbGqlMzwZ0cwsmwNJL9Q3ZtZqd9eW\nmVmGA0kv1HXl2XKLxMwsw4GkFzKZfz2PxMxsHweSXnB6FDOzAzmQ9EJ9QwsSjK9wIDEzy3Ag6YW6\nxlYmVJRRMkyFroqZ2YDhQNIL9Q2e1W5m1p0DSS9sb2z1+IiZWTcOJL1Q3+D0KGZm3TmQ9EJdQwvV\nbpGYme3HgSRHre2d7G5ud4vEzKwbB5Ic7WjKpEdxi8TMLJsDSY660qO4a8vMbD8OJDnKzGp315aZ\n2f4cSHJU3+gWiZlZTxxIcuQWiZlZzxxIclTf2MrwEjGmPK+LSpqZDToOJDmqb2hhwqgyJOfZMjPL\n5kCSo/qGVqpGuVvLzKw7B5Ic1TW2eg6JmVkPHEhyVN/Q4pURzcx64ECSI2f+NTPrmQNJDppa22lq\n7XDXlplZDxxIcpCZQ1LtwXYzswM4kOSgvtEJG83MDsaBJAf1mYSNHmw3MzuAA0kOulokHmw3MztA\nXgOJpPMkrZG0VtLneyifJeleSSsl3S9pelbZTEl3S1ot6WlJs9PtcyQ9kp7zF5Ly/tt9X54tBxIz\ns+7yFkgklQDXAOcDC4BLJS3otts3gBsiYiFwNfD1rLIbgH+JiBOA04GX0u3/DHwrIuYBO4AP5usa\nMuobWhg5vISKMufZMjPrLp8tktOBtRGxLiJagRuBt3XbZwHw+/T9fZnyNOCURsQ9ABHREBFNShJd\nnQvclB5zPXBhHq8BSLq23BoxM+tZPgPJNGBj1udN6bZsK4CL0vdvByolVQEvA3ZK+rWkxyX9S9rC\nqQJ2RkT7Ic4JgKTLJS2XtHzbtm19upAkkHig3cysJ4UebL8SWCLpcWAJsBnoAEqBs9LyVwBzgct6\nc+KIuDYiFkfE4okTJ/apkvUNLR5oNzM7iHwGks3AjKzP09NtXSKiJiIuiohTgS+k23aStDSeSLvF\n2oHfAKcB9cA4SaUHO2c+JJl/HUjMzHqSz0DyKDA/fcqqDLgEuCV7B0nVkjJ1uAq4LuvYcZIyTYlz\ngacjIkjGUt6Zbn8f8Ns8XgMRQX1ji7u2zMwOIm+BJG1JfBy4C1gN/DIinpJ0taS3prudA6yR9Cww\nGfhaemwHSbfWvZJWAQJ+kB7zOeBTktaSjJn8Z76uAWB3czttHUG1B9vNzHqU1+dZI+J24PZu276Y\n9f4m9j2B1f3Ye4CFPWxfR/JE2FGxPZ2M6My/ZmY9K/Rg+4Dn9ChmZofmQHIYdQ1Oj2JmdigOJIdR\n35i0SLw6oplZzxxIDiOTZ8tjJGZmPXMgOYztja1UlpdSVuoflZlZT/zb8TDqGlrcrWVmdggOJIfh\nWe1mZofmQHIYyax2BxIzs4NxIDmM+gZn/jUzOxQHkkPo6Ax2NLlry8zsUBxIDmFnUyud4cmIZmaH\n4kByCPWNmbXa3bVlZnYwDiSHUNeVZ8stEjOzg3EgOYRM5l/PIzEzOzgHkkNwehQzs8NzIDmE+oYW\nJBhf4UBiZnYwDiSHUNfYyoSKMkqGqdBVMTMbsBxIDqG+wbPazcwOJ69L7Q52C6ePY+7E0YWuhpnZ\ngOZAcggfe+28QlfBzGzAc9eWmZn1iQOJmZn1iQOJmZn1iQOJmZn1iQOJmZn1iQOJmZn1iQOJmZn1\niQOJmZn1iSKi0HXIO0nbgBe6ba4G6gpQnUIrxuv2NRePYrzufF7zrIiYeLidiiKQ9ETS8ohYXOh6\nHG3FeN2+5uJRjNc9EK7ZXVtmZtYnDiRmZtYnxRxIri10BQqkGK/b11w8ivG6C37NRTtGYmZm/aOY\nWyRmZtYPHEjMzKxPijKQSDpP0hpJayV9vtD16S+SZki6T9LTkp6SdEW6fYKkeyQ9l34dn26XpO+k\nP4eVkk4r7BUcOUklkh6XdGv6eY6kR9Jr+4WksnT7iPTz2rR8diHr3ReSxkm6SdIzklZLOnOo32tJ\nn0z/bT8p6eeSyofivZZ0naSXJD2Zta3X91bS+9L9n5P0vnzVt+gCiaQS4BrgfGABcKmkBYWtVb9p\nBz4dEQuAM4CPpdf2eeDeiJgP3Jt+huRnMD99XQ587+hXud9cAazO+vzPwLciYh6wA/hguv2DwI50\n+7fS/QarbwN3RsTxwCKS6x+y91rSNOATwOKIOAkoAS5haN7rHwPnddvWq3sraQLwJeCVwOnAlzLB\np99FRFG9gDOBu7I+XwVcVeh65elafwu8AVgDTEm3TQHWpO+/D1yatX/XfoPpBUxP/2OdC9wKiGSm\nb2n3ew7cBZyZvi9N91Ohr+EIrnkssL573YfyvQamARuBCem9uxV401C918Bs4MkjvbfApcD3s7bv\nt19/voquRcK+f4wZm9JtQ0rajD8VeASYHBG1adEWYHL6fqj8LP4N+CzQmX6uAnZGRHv6Ofu6uq45\nLd+V7j/YzAG2AT9Ku/R+KGkUQ/heR8Rm4BvAi0Atyb17jKF/rzN6e2+P2j0vxkAy5EkaDdwM/F1E\n7M4ui+RPkyHzzLekC4CXIuKxQtflKCsFTgO+FxGnAo3s6+oAhuS9Hg+8jSSITgVGcWD3T1EYaPe2\nGAPJZmBG1ufp6bYhQdJwkiDys4j4dbp5q6QpafkU4KV0+1D4WbwaeKukDcCNJN1b3wbGSSpN98m+\nrq5rTsvHAvVHs8L9ZBOwKSIeST/fRBJYhvK9fj2wPiK2RUQb8GuS+z/U73VGb+/tUbvnxRhIHgXm\np096lJEM1t1S4Dr1C0kC/hNYHRHfzCq6Bcg8sfE+krGTzPa/TZ/6OAPYldV0HhQi4qqImB4Rs0nu\n5e8j4t3AfcA70926X3PmZ/HOdP8B85ddriJiC7BR0nHpptcBTzOE7zVJl9YZkirSf+uZax7S9zpL\nb+/tXcAbJY1PW3NvTLf1v0IPKBVoEOvNwLPA88AXCl2ffryu15A0d1cCT6SvN5P0C98LPAf8DpiQ\n7i+SJ9ieB1aRPA1T8Ovow/WfA9yavp8L/AVYC/wKGJFuL08/r03L5xa63n243lOA5en9/g0wfqjf\na+ArwDPAk8BPgBFD8V4DPycZB2ojaX1+8EjuLfCB9PrXAu/PV32dIsXMzPqkGLu2zMysHzmQmJlZ\nnziQmJlZnziQmJlZnziQmJlZnziQWNGTNDs7y2q3sqslvb6H7edkMg33ULZBUnV/1zMX2fWV9HeS\nKgpRDysupYffxax4RcQXC12H3uhW378Dfgo0Fag6ViTcIjFLlEj6QbrWxd2SRgJI+rGkd6bvz0vX\n/ngIuChzoKSq9JjHJX2fZIJYpuw9kv4i6QlJ30+XMUBSg6SvSVoh6WFJk+lG0pclXZn1+cm09TRb\nyfojB62vpE+Q5KO6T8kaNSVp2ZOSVkn6ZH5+jFaMHEjMEvOBayLiRGAn8I7sQknlwA+ApcBZwDFZ\nxV8CHookeeItwMz0mBOAvwFeHRGnAB3Au9NjRgEPR8Qi4EHgw/1Z34j4DlADvDYiXksyC35aRJwU\nEScDP+rl9zM7KAcSs8T6iHgiff8YyVoQ2Y5P93kuknQQP80qOzvzOSJuI1lcCZJcUC8HHpX0RPp5\nblrWSrKexsG+X1/r2906YK6k70o6D9h9mP3NcuYxErNES9b7DmBkP5xTwPURcVUPZW2xLz9RBz3/\nX2xn/z/2yrPe96q+EbFD0iKShaA+BlxMkofJrM/cIjHLzTPAbEnHpp8vzSp7EHgXgKTzSZInQpJg\n752SJqVlEyTN6sX33ECSGp50He45vazzHqAyPb4aGBYRNwP/kDmvWX9wIDHLQUQ0k6yHfVs62P5C\nVvFXgLMl/ZUkVfeL6TFPA38P3C1pJXAPyRKouboZmCDpceCjJBmre+Na4E5J95GsjHd/2sX2Y5Il\nps36hbP/mplZn7hFYmZmfeJAYmZmfeJAYmZmfeJAYmZmfeJAYmZmfeJAYmZmfeJAYmZmffL/AYHy\ng65NwHxhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x263762a518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(hidden_units,accuracies)\n",
    "\n",
    "plt.title('hidden units accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('hidden units')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
